{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855db878-7efb-4c37-af68-4511bed1dc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3cfb4c-c16c-4d03-8697-02a8fecae80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Apache Spark is a unified computing engine and a set of libraries for parallel data processing on\n",
    "computer clusters.\n",
    "\n",
    "Why Unified: \n",
    "Spark is called unified because a single engine + a single execution model supports many different data processing workloads, instead of needing separate systems for each one.\n",
    "\n",
    "Before Spark, you typically needed different tools for different jobs\n",
    "\n",
    "Workload\t        Old-school tool\n",
    "Batch processing\tMapReduce\n",
    "SQL queries\t        Hive\n",
    "Streaming\t        Storm\n",
    "Machine learning\tMahout\n",
    "Graph processing\tGiraph\n",
    "\n",
    "Each had: Its own engine, Its own APIs, Its own data movement and Lots of glue code.\n",
    "\n",
    "Spark provides one core engine (Spark Core) and builds everything else on top of it:\n",
    "Spark SQL, Structured streaming, MLlib,Graphx.\n",
    "\n",
    "In practice, unified means, you use one pipeline end to end, one execution and optimization engine, \n",
    "one programming model (for different workloads like sql, Ml etc), and One fault-tolerance & scheduling layer\n",
    "\n",
    "Compute Engine: Spark is only compute engine, not platform (Databricks is, again only Big Data platform, not fullfledged app. platform), whereas Hadoop, an older sibling in Big data toolset,\n",
    "designed to be performant by storing data local to it. Spark is designed to be distributed computing engine, it reads data from variety of location inlcuding cloud storages and performs\n",
    "computations and returns/writes the data back.\n",
    "\n",
    "Librabries: As a unified engine to provide a unified API for common data analysis tasks. Spark supports both standard libraries that ship with the engine as well as a wide array of \n",
    "external libraries published as third-party packages by the open source communities.\n",
    "\n",
    "Spark Architecture:\n",
    "\n",
    "Component\t    Analogy\t                  Real-World Function\n",
    "Cluster\t        The Factory\t              The physical group of servers.\n",
    "Driver\t        The Project Manager\t      Coordinates work and talks to the user.\n",
    "SparkSession\tThe Project Contract\t  The interface used to start work.\n",
    "Executors\t    The Factory Workers       Perform the actual data processing.\n",
    "\n",
    "\n",
    "Apache Spark follows a master–worker architecture designed for distributed data processing.\n",
    "\n",
    "SparkSession is the entry point to Spark. It allows users to create DataFrames, run SQL, and configure Spark. It is a handle to Spark, not an execution component.\n",
    "\n",
    "The Driver is the brain of a Spark application. It runs the main program, creates the logical execution plan (DAG), splits it into stages and tasks, schedules those tasks,\n",
    "and tracks execution and failures. There is exactly one driver per application.\n",
    "\n",
    "A Cluster is the set of machines where Spark runs. It provides compute resources (CPU and memory) but does not contain Spark logic itself.\n",
    "\n",
    "The Cluster Manager (YARN, Kubernetes, Standalone, etc.) is responsible for allocating resources and launching executors on worker nodes.\n",
    "\n",
    "Executors are worker JVM processes that run on cluster nodes. They execute tasks on data partitions, perform transformations, handle shuffles, \n",
    "and store intermediate or cached data in memory or disk.\n",
    "\n",
    "Overall, Spark separates coordination (driver), resource management (cluster manager), and execution (executors), while relying on external storage for data persistence.\n",
    "This design enables scalable, fault-tolerant, and flexible distributed computation.\n",
    "\n",
    "Step-by-step flow\n",
    "\n",
    "User writes Spark code ->  SparkSession is created -> Driver starts -> Driver contacts cluster manager -> Cluster manager launches executors\n",
    "Driver builds DAG -> (DAG → stages → tasks) -> Tasks sent to executors -> Executors process partitions -> Results sent back to driver or written to storage.\n",
    "\n",
    "DAG = what needs to be done (logical plan)\n",
    "\n",
    "Stages = how the work is broken at shuffle boundaries (narrow and wide transformations, depends on code, stages are created)\n",
    "\n",
    "Tasks = actual parallel work on data partitions\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2e9a08-e405-4245-ac17-51b1a019c601",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Spark APIs: spark APIs let spark to run in various languages like Scala, java, Python, R, SQL. These language APIs can be used to drive the Spark or invoke the Driver.\n",
    "Spark has two fundamental sets of APIs: the low-level “unstructured” APIs, and the higher-level structured APIs.\n",
    "\n",
    "Dataframe: Represents a table of data with rows and columns, is part of structured API. Spark's Dataframe is distributed unlike Python's Panda's DF or R's Dataframe.\n",
    "The dataframe data is partitioned in Spark called Input Partitions or Dataframe partitions, this is generally 128MB per partition. (Shuffle partition depends on number of pertitions setting)\n",
    "\n",
    "Transformations: the core data structures are immutable, meaning they cannot be changed after they’re created. To “change” a DataFrame, you need to instruct Spark how you would like to\n",
    "modify it to do what you want. These instructions are called transformations.\n",
    "Ex:\n",
    "divisBy2 = myRange.where(\"number % 2 = 0\")\n",
    "\n",
    "There are two types of transformations: those that specify narrow dependencies, and those that specify wide dependencies.\n",
    "\n",
    "When transformation depends on its own partition, then narrow (1:1 partitions)(ex: Filter), when require multiple partitions to create a new partition then wide (1:N partitions) \n",
    "(ex: Group By, sort).\n",
    "\n",
    "Actions: An action instructs Spark to compute a result from a series of transformations. Ex: Compute, collect, show, head etc.\n",
    "\n",
    "> Actions to view data in the console\n",
    "> Actions to collect data to native objects in the respective language\n",
    "> Actions to write to output data sources\n",
    "\n",
    "Lazy Evaluation: Lazy Evaluation means that Spark does not execute your code immediately when you write it. Instead, it records your instructions as a \"plan\" and only runs them \n",
    "when you explicitly ask for a final result.\n",
    "\n",
    "Transformations (Lazy): ilter(), map(), select(), groupBy(), join(). They return a new DataFrame but the data inside remains untouched. Spark just adds a \"step\" to the DAG (Lineage).\n",
    "Why Lazy is good: Example: The \"Filter Pushdown\"\n",
    "\n",
    "Imagine you have a 10TB dataset, and your code looks like this:\n",
    "\n",
    "    Read the 10TB dataset.\n",
    "\n",
    "    Select 2 columns.\n",
    "\n",
    "    Filter for ID = 5.\n",
    "\n",
    "    Show the result.\n",
    "    If Spark was Eager: It would load all 10TB into memory, then throw away 99% of the columns, then throw away 99.9% of the rows. It would likely crash.\n",
    "\n",
    "Because Spark is Lazy: It looks at the whole plan. It realizes it only needs one specific ID. It tells the data source (like Parquet or a Database) to only send the data where ID=5. \n",
    "It never loads the 10TB. This is called Predicate Pushdown.\n",
    "\n",
    "Spark UI: To monitor the Jobs. localhost:4040\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "071d21f1-1a88-4351-842d-a6c9762f5cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/31 19:06:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.5:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkInJupyter</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x70c536890770>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkInJupyter\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9abf54d2-c756-4547-8a00-45563f2bc813",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'getpwd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m myRange \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mrange(\u001b[38;5;241m1000\u001b[39m)\u001b[38;5;241m.\u001b[39mtoDF(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mgetpwd\u001b[49m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'getpwd' is not defined"
     ]
    }
   ],
   "source": [
    "myRange = spark.range(1000).toDF(\"number\")\n",
    "getpwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bbd5854-a57d-438a-ac73-14b1f7534893",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015 = spark\\\n",
    ".read\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".csv(\"./SparkBasics/2015-summary.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a33225c-87b2-47dd-8a45-218273aed3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "|     5|\n",
      "|     6|\n",
      "|     7|\n",
      "|     8|\n",
      "|     9|\n",
      "|    10|\n",
      "|    11|\n",
      "|    12|\n",
      "|    13|\n",
      "|    14|\n",
      "|    15|\n",
      "|    16|\n",
      "|    17|\n",
      "|    18|\n",
      "|    19|\n",
      "+------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "divisBy2 = myRange.where(\"number % 2 = 0\")\n",
    "print(divisBy2.count())\n",
    "myRange.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c34f873-6c1b-4a39-b6e2-cb70ab2af503",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015.show()\n",
    "print(flightData2015.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8625b066-adea-4819-ae88-9ae1b8456d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015.sort(\"count\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632a3de2-aa98-4ea8-b1eb-160981f7bf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015.sort(\"count\").show(flightData2015.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa55de4-1ef1-48d6-88a9-f3ea2c61f95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b62eba0-1186-4c64-b5c5-bc72db91f0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "collectDF=flightData2015.sort(\"count\").collect() # collects data to native programming interface data structure, since pyspark, brings everything from all teh executors to List.\n",
    "# Chances are there that  spark crashes, because the driver node may have say 512MB of memory and data collected from all executors may be over 512MB, ion that case OOO will come and crashed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9997ca7-61a1-4a48-98ce-1100e1eb8174",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(collectDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a2e03d-25a9-4130-a1a4-74c9913a95f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Partitions are dictated by the size of the data on disk (up to 128MB).\n",
    "\n",
    "# Shuffle Partitions are dictated by your config settings, regardless of how small the input data is.\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136e5ec6-7fc1-40e6-a7bd-786301d42cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015.createOrReplaceTempView(\"flight_data_2015\") # create a view from Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38566327-9072-4ad1-8c6a-4d6511364691",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlWay = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, count(1)\n",
    "FROM V_flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\")\n",
    "dataFrameWay = flightData2015\\\n",
    ".groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    ".count()\n",
    "sqlWay.explain()\n",
    "dataFrameWay.explain()\n",
    "\n",
    "sqlWay.show()\n",
    "dataFrameWay.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd4c6ac-3746-464b-91ed-0bf2454ef726",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT max(count) from V_flight_data_2015\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0a65fe-74c7-43e3-a2a8-2eab190eeef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max\n",
    "flightData2015.select(max(\"count\")).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b87f41-dd03-4878-808e-6bcde5fcaf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 5 destination countries\n",
    "maxSql = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n",
    "FROM V_flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "ORDER BY sum(count) DESC\n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "maxSql.show()\n",
    "#Df way\n",
    "from pyspark.sql.functions import desc\n",
    "flightData2015\\\n",
    ".groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    ".sum(\"count\")\\\n",
    ".withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
    ".sort(desc(\"destination_total\"))\\\n",
    ".limit(5)\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3a08d1-9e93-4309-aef3-a21e879c64b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc32f958-0c85-4ca1-bed0-31fb74edaf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                            # Structured APIs\n",
    "# Chapter 4. Structured API Overview\n",
    "'''\n",
    "The Structured APIs are a tool for manipulating all sorts of data, from unstructured log files to semi-structured CSV files and highly structured Parquet files. \n",
    "These APIs refer to three core types of distributed collection APIs:\n",
    "1. Datasets\n",
    "2. DataFrames\n",
    "3. SQL tables and views\n",
    "\n",
    "Note: Spark is a distributed programming model in which the user specifies transformations. Multiple transformations build up a directed acyclic graph of instructions. \n",
    "An action begins the process ofexecuting that graph of instructions, as a single job, by breaking it down into stages and tasks to execute across the cluster. \n",
    "The logical structures that we manipulate with transformations and actions are DataFrames and Datasets.\n",
    "To create a new DataFrame or Dataset, you call a transformation. To start computation or convert to native language types, you call an action.\n",
    "\n",
    "DataFrames and Datasets: DataFrames and Datasets are (distributed) table-like (i.e. structured) collections with well-defined rows and columns.\n",
    "Tables and views are basically the same thing as DataFrames. We just execute SQL against them instead of DataFrame code\n",
    "\n",
    "Schemas: A schema defines the column names and types of a DataFrame. You can define schemas manually or read a schema from a data source (often called schema on read). Schemas consist of\n",
    "types, meaning that you need a way of specifying what lies where.\n",
    "\n",
    "Dataframe vs Datasets: DF is untyped, though, columns have types, but they are evaluated at run time, whereas for DS, should be defined first. Therefore, for SchemaOnRead type,\n",
    "with only few records of data, there could be chance that data type may not accurately represent the run time value. say first 5 records on read of schema has all decimal values but later \n",
    "some char values. Data sets are only available in Scala and Java (JVM based). \n",
    "\n",
    "Spark has it own datatypes internally as its a programming language too. ex: ByteType, IntegerType, LongType,DateType etc.\n",
    "\n",
    "Overview of Structured API Execution: \n",
    "1. Write DataFrame/Dataset/SQL Code.\n",
    "2. If valid code, Spark converts this to a Logical Plan.\n",
    "3. Spark transforms this Logical Plan to a Physical Plan, checking for optimizations along\n",
    "the way.\n",
    "4. Spark then executes this Physical Plan (RDD manipulations) on the cluster.\n",
    "\n",
    "Execution: Upon selecting a physical plan, Spark runs all of this code over RDDs, the lower-level programming interface of Spark. Spark performs further\n",
    "optimizations at runtime, generating native Java bytecode that can remove entire tasks or stages during execution. Finally the result is returned to the user.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31a055e-bc9f-455e-a152-93a371fd4f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chapter 5. Basic Structured Operations\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkInJupyter\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fbc785b-b5ef-4e7e-a62b-5b580160a0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('DEST_COUNTRY_NAME', StringType(), True), StructField('ORIGIN_COUNTRY_NAME', StringType(), True), StructField('count', LongType(), True)])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format(\"json\").load(\"./SparkBasics/data/flight-data/json/2015-summary.json\")\n",
    "spark.read.format(\"json\").load(\"./SparkBasics/data/flight-data/json/2015-summary.json\").schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623072fd-b1c1-4f13-9935-aea3548ee522",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "myManualSchema = StructType([\n",
    "  StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
    "  StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
    "  StructField(\"count\", LongType(), False, metadata={\"hello\":\"world\"})\n",
    "])\n",
    "df = spark.read.format(\"json\").schema(myManualSchema)\\\n",
    "  .load(\"./data/flight-data/json/2015-summary.json\")\n",
    "df.show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf8adc8-e8f4-4b00-856c-c0c753d97925",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, column\n",
    "col(\"someColumnName\")\n",
    "column(\"someColumnName\")\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "expr(\"(((someCol + 5) * 200) - 6) < otherCol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db902037-b76b-48e3-bf8e-f416d2abd5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import Row\n",
    "myRow = Row(\"Hello\", None, 1, False)\n",
    "\n",
    "myRow[0]\n",
    "myRow[2]\n",
    "\n",
    "df = spark.read.format(\"json\").load(\"./data/flight-data/json/2015-summary.json\")\n",
    "df.createOrReplaceTempView(\"dfTable\")\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c17214-2cb6-4107-898c-664440254d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "myManualSchema = StructType([\n",
    "  StructField(\"some\", StringType(), True),\n",
    "  StructField(\"col\", StringType(), True),\n",
    "  StructField(\"names\", LongType(), False)\n",
    "])\n",
    "myRow = Row(\"Hello\", None, 1)\n",
    "myDf = spark.createDataFrame([myRow], myManualSchema)\n",
    "myDf.show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1b420a-62ba-496b-9d82-850ec0a4cb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df.select(\"DEST_COUNTRY_NAME\").show(2)\n",
    "df.select(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\").show(2)\n",
    "\n",
    "from pyspark.sql.functions import expr, col, column\n",
    "df.select(\n",
    "    expr(\"DEST_COUNTRY_NAME\"),\n",
    "    col(\"DEST_COUNTRY_NAME\"),\n",
    "    column(\"DEST_COUNTRY_NAME\"))\\\n",
    "  .show(2)\n",
    "\n",
    "\n",
    "df.select(expr(\"DEST_COUNTRY_NAME AS destination\")).show(2)\n",
    "\n",
    "df.select(expr(\"DEST_COUNTRY_NAME as destination\").alias(\"DEST_COUNTRY_NAME\"))\\\n",
    "  .show(2)\n",
    "\n",
    "df.selectExpr(\"DEST_COUNTRY_NAME as newColumnName\", \"DEST_COUNTRY_NAME\").show(2)\n",
    "\n",
    "df.selectExpr(\n",
    "  \"*\", # all original columns\n",
    "    \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\")\n",
    "  .show(2)\n",
    "\n",
    "df.selectExpr(\"avg(count)\", \"count(distinct(DEST_COUNTRY_NAME))\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc44a3d-83ff-4110-9885-8a4ee5245332",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import lit\n",
    "df.select(expr(\"*\"), lit(1).alias(\"One\")).show(2)\n",
    "\n",
    "df.withColumn(\"numberOne\", lit(1)).show(2)\n",
    "\n",
    "df.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\")).show(2)\n",
    "\n",
    "df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"dest\").columns\n",
    "\n",
    "dfWithLongColName = df.withColumn(\n",
    "    \"This Long Column-Name\",\n",
    "    expr(\"ORIGIN_COUNTRY_NAME\"))\n",
    "\n",
    "dfWithLongColName.selectExpr(\n",
    "    \"`This Long Column-Name`\",\n",
    "    \"`This Long Column-Name` as `new col`\").show(2)\n",
    "\n",
    "dfWithLongColName.select(expr(\"`This Long Column-Name`\")).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b63ea4-56de-4fde-8033-06a23b7d61a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.where(col(\"count\") < 2).where(col(\"ORIGIN_COUNTRY_NAME\") != \"Croatia\").show(2)\n",
    "\n",
    "df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().count()\n",
    "\n",
    "df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count()\n",
    "\n",
    "seed = 5\n",
    "withReplacement = False\n",
    "fraction = 0.5\n",
    "df.sample(withReplacement, fraction, seed).count()\n",
    "\n",
    "dataFrames = df.randomSplit([0.25, 0.75], seed)\n",
    "dataFrames[0].count() > dataFrames[1].count() # False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c93944b-67de-4b21-943a-20c78d2d3e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import Row\n",
    "schema = df.schema\n",
    "newRows = [\n",
    "  Row(\"New Country\", \"Other Country\", 5),\n",
    "  Row(\"New Country 2\", \"Other Country 3\", 1)\n",
    "]\n",
    "parallelizedRows = spark.sparkContext.parallelize(newRows)\n",
    "newDF = spark.createDataFrame(parallelizedRows, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4ae66-7058-45c1-b2c2-9414d7882126",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.union(newDF)\\\n",
    "  .where(\"count = 1\")\\\n",
    "  .where(col(\"ORIGIN_COUNTRY_NAME\") != \"United States\")\\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13be6d70-10ae-4adf-8636-d8bc998b5ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.sort(\"count\").show(5)\n",
    "df.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(5)\n",
    "df.orderBy(col(\"count\"), col(\"DEST_COUNTRY_NAME\")).show(5)\n",
    "\n",
    "from pyspark.sql.functions import desc, asc\n",
    "df.orderBy(expr(\"count desc\")).show(2)\n",
    "df.orderBy(col(\"count\").desc(), col(\"DEST_COUNTRY_NAME\").asc()).show(2)\n",
    "\n",
    "spark.read.format(\"json\").load(\"./data/flight-data/json/*-summary.json\")\\\n",
    "  .sortWithinPartitions(\"count\")\n",
    "df.limit(5).show()\n",
    "df.orderBy(expr(\"count desc\")).limit(6).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7a6655-3ae3-4266-ae04-1a26a423234e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.getNumPartitions() # 1\n",
    "df.repartition(5)\n",
    "df.repartition(col(\"DEST_COUNTRY_NAME\"))\n",
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\"))\n",
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b5f581-ccd9-4009-a0b1-16ec75144872",
   "metadata": {},
   "outputs": [],
   "source": [
    "collectDF = df.limit(10)\n",
    "collectDF.take(5) # take works with an Integer count\n",
    "collectDF.show() # this prints it out nicely\n",
    "collectDF.show(5, False)\n",
    "collectDF.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc32f958-0c85-4ca1-bed0-31fb74edaf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                            # Structured APIs\n",
    "# Chapter 4. Structured API Overview\n",
    "'''\n",
    "The Structured APIs are a tool for manipulating all sorts of data, from unstructured log files to semi-structured CSV files and highly structured Parquet files. \n",
    "These APIs refer to three core types of distributed collection APIs:\n",
    "1. Datasets\n",
    "2. DataFrames\n",
    "3. SQL tables and views\n",
    "\n",
    "Note: Spark is a distributed programming model in which the user specifies transformations. Multiple transformations build up a directed acyclic graph of instructions. \n",
    "An action begins the process ofexecuting that graph of instructions, as a single job, by breaking it down into stages and tasks to execute across the cluster. \n",
    "The logical structures that we manipulate with transformations and actions are DataFrames and Datasets.\n",
    "To create a new DataFrame or Dataset, you call a transformation. To start computation or convert to native language types, you call an action.\n",
    "\n",
    "DataFrames and Datasets: DataFrames and Datasets are (distributed) table-like (i.e. structured) collections with well-defined rows and columns.\n",
    "Tables and views are basically the same thing as DataFrames. We just execute SQL against them instead of DataFrame code\n",
    "\n",
    "Schemas: A schema defines the column names and types of a DataFrame. You can define schemas manually or read a schema from a data source (often called schema on read). Schemas consist of\n",
    "types, meaning that you need a way of specifying what lies where.\n",
    "\n",
    "Dataframe vs Datasets: DF is untyped, though, columns have types, but they are evaluated at run time, whereas for DS, should be defined first. Therefore, for SchemaOnRead type,\n",
    "with only few records of data, there could be chance that data type may not accurately represent the run time value. say first 5 records on read of schema has all decimal values but later \n",
    "some char values. Data sets are only available in Scala and Java (JVM based). \n",
    "\n",
    "Spark has it own datatypes internally as its a programming language too. ex: ByteType, IntegerType, LongType,DateType etc.\n",
    "\n",
    "Overview of Structured API Execution: \n",
    "1. Write DataFrame/Dataset/SQL Code.\n",
    "2. If valid code, Spark converts this to a Logical Plan.\n",
    "3. Spark transforms this Logical Plan to a Physical Plan, checking for optimizations along\n",
    "the way.\n",
    "4. Spark then executes this Physical Plan (RDD manipulations) on the cluster.\n",
    "\n",
    "Execution: Upon selecting a physical plan, Spark runs all of this code over RDDs, the lower-level programming interface of Spark. Spark performs further\n",
    "optimizations at runtime, generating native Java bytecode that can remove entire tasks or stages during execution. Finally the result is returned to the user.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31a055e-bc9f-455e-a152-93a371fd4f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chapter 5. Basic Structured Operations\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkInJupyter\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbc785b-b5ef-4e7e-a62b-5b580160a0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"json\").load(\"./data/flight-data/json/2015-summary.json\")\n",
    "spark.read.format(\"json\").load(\"./data/flight-data/json/2015-summary.json\").schema # cehck relative path from where spark application builder located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b89e51-109c-4dd2-961f-57b5a927e6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of DataFrames: different ways of creating DataFrame in PySpark.\n",
    "#1. From a List of Rows or Tuples:\n",
    "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Charlie\", 29)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "df.show()\n",
    "#2. From an Existing RDD.\n",
    "'''\n",
    "Using toDF(): A quick way to convert an RDD.\n",
    "\n",
    "Using createDataFrame(rdd): Gives you more control over the schema.\n",
    "\n",
    "3. Reading from Files:\n",
    "spark.read.csv(\"path.csv\", header=True)\n",
    "spark.read.parquet(\"path.parquet\")\n",
    "spark.read.json(\"path.json\")\n",
    "\n",
    "4. from Pandas DataFrame:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pdf = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]})\n",
    "sdf = spark.createDataFrame(pdf)\n",
    "\n",
    "5. From External Database:\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/db\") \\\n",
    "    .option(\"dbtable\", \"employees\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .load()\n",
    "\n",
    "6. Using Spark SQL Queries:\n",
    "spark.sql(\"CREATE OR REPLACE TEMPORARY VIEW people AS SELECT 'John' as name, 30 as age\")\n",
    "df = spark.sql(\"SELECT * FROM people WHERE age > 25\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623072fd-b1c1-4f13-9935-aea3548ee522",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A schema is a StructType made up of a number of fields, StructFields, that have a name,\n",
    "type, a Boolean flag which specifies whether that column can contain missing or null values,\n",
    "and, finally, users can optionally specify associated metadata with that column. The metadata is a\n",
    "way of storing information about this column (Spark uses this in its machine learning library).\n",
    "'''\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "myManualSchema = StructType([\n",
    "  StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
    "  StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
    "  StructField(\"count\", LongType(), False, metadata={\"hello\":\"world\"})\n",
    "])\n",
    "df = spark.read.format(\"json\").schema(myManualSchema)\\\n",
    "   .load(\"./data/flight-data/json/2015-summary.json\")\n",
    "df.show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf8adc8-e8f4-4b00-856c-c0c753d97925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns: Column can be defined individually , but value of column exists as part of row, therefore as part of a dataframe.\n",
    "from pyspark.sql.functions import col, column, expr, lit, concat, when\n",
    "\n",
    "col(\"someColumnName\")\n",
    "column(\"someColumnName\")\n",
    "print(col(\"someColumnName\"),column(\"someColumnName\") )\n",
    "\n",
    "#An expression is a set of transformations on one or more values in a record in a DataFrame.\n",
    "expr(\"(((someCol + 5) * 200) - 6) < otherCol\") \n",
    "\n",
    "# Demonstration of modularity with columns and expressions outside Dataframe upfront.\n",
    "#1. Define your logic UP FRONT (outside the context of any data)\n",
    "# These are just \"blueprints\" for calculations\n",
    "is_adult_col = (col(\"age\") >= 18)\n",
    "\n",
    "status_desc_col = when(col(\"age\") >= 18, \"Adult\") \\\n",
    "                  .otherwise(\"Minor\") \\\n",
    "                  .alias(\"life_stage\")\n",
    "\n",
    "full_info_col = concat(col(\"name\"), lit(\" - \"), col(\"age\")).alias(\"profile_summary\")\n",
    "\n",
    "# 2. Setup Spark and dummy data\n",
    "spark = SparkSession.builder.appName(\"ColumnDemo\").getOrCreate()\n",
    "data = [(\"Alice\", 25), (\"Bob\", 12), (\"Charlie\", 30)]\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
    "\n",
    "# 3. Apply those pre-defined columns to the DataFrame\n",
    "# Spark matches the string names inside col() to the DF schema now\n",
    "final_df = df.select(\n",
    "    \"name\",\n",
    "    \"age\",\n",
    "    is_adult_col.alias(\"is_adult\"),\n",
    "    status_desc_col,\n",
    "    full_info_col\n",
    ")\n",
    "\n",
    "final_df.show()\n",
    "\n",
    "# accessign the dataframe columns\n",
    "spark.read.format(\"json\").load(\"./data/flight-data/json/2015-summary.json\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c17214-2cb6-4107-898c-664440254d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows:\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "myManualSchema = StructType([\n",
    "  StructField(\"some\", StringType(), True),\n",
    "  StructField(\"col\", StringType(), True),\n",
    "  StructField(\"names\", LongType(), False)\n",
    "])\n",
    "myRow = Row(\"Hello\", None, 1)\n",
    "print('Row Values: ', myRow[0], myRow[2])\n",
    "myDf = spark.createDataFrame([myRow], myManualSchema)\n",
    "myDf.show(1) # returns n records to console\n",
    "myDf.first() # returns first n records to program control, use object, say column object to access it,ex:\n",
    "print(myDf.first()[\"some\"])\n",
    "myDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f71c94d-469e-4333-b3c2-d0994f2bba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations on DataFrames:\n",
    "'''\n",
    "We can add rows or columns\n",
    "We can remove rows or columns\n",
    "We can transform a row into a column (or vice versa)\n",
    "We can change the order of rows based on the values in columns\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1b420a-62ba-496b-9d82-850ec0a4cb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"json\").load(\"./data/flight-data/json/2015-summary.json\")\n",
    "df.show(1)\n",
    "\n",
    "\n",
    "df.select(\"DEST_COUNTRY_NAME\").show(2)\n",
    "df.select(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\").show(2)\n",
    "\n",
    "from pyspark.sql.functions import expr, col, column\n",
    "df.select(expr(\"DEST_COUNTRY_NAME\"),col(\"DEST_COUNTRY_NAME\"),column(\"DEST_COUNTRY_NAME\")).show(2)\n",
    "\n",
    "\n",
    "df.select(expr(\"DEST_COUNTRY_NAME AS destination\")).show(2)\n",
    "\n",
    "df.select(expr(\"DEST_COUNTRY_NAME as destination\").alias(\"DEST_COUNTRY_NAME\")).show(2)\n",
    "df.select(expr(\"'The '|| DEST_COUNTRY_NAME as `DEST CTY`\")).show(1)\n",
    "\n",
    "'''\n",
    "We can treat selectExpr as a simple way to build up\n",
    "complex expressions that create new DataFrames. In fact, we can add any valid non-aggregating\n",
    "SQL statement, and as long as the columns resolve, it will be valid! '''\n",
    "\n",
    "df.selectExpr(\"DEST_COUNTRY_NAME as newColumnName\", \"DEST_COUNTRY_NAME\").show(2)\n",
    "\n",
    "df.selectExpr(\n",
    "  \"*\", # all original columns\n",
    "    \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\").show(2)\n",
    "\n",
    "df.selectExpr(\"avg(count)\", \"count(distinct(DEST_COUNTRY_NAME))\").show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370f8e52-e2ec-4eca-89e8-6b64034afac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark SQL:\n",
    "df.createOrReplaceTempView(\"dfTable\") # Creata a Temp. view to register in catalog of Spark for SQL queries\n",
    "spark.sql(\"select count from dfTable\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc44a3d-83ff-4110-9885-8a4ee5245332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new columns\n",
    "from pyspark.sql.functions import lit # lit is for 'literal'\n",
    "\n",
    "df.select(expr(\"*\"), lit(1).alias(\"One\")).show(2)\n",
    "\n",
    "df.withColumn(\"numberOne\", lit(1)).show(2)\n",
    "\n",
    "df.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\")).show(2)\n",
    "\n",
    "#Renaming Columns\n",
    "\n",
    "df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"dest\").columns\n",
    "\n",
    "# Handling reserved characters: use back-tick or single quote with 'withColumn'\n",
    "dfWithLongColName = df.withColumn(\n",
    "    \"This Long Column-Name\",\n",
    "    expr(\"ORIGIN_COUNTRY_NAME\"))\n",
    "\n",
    "dfWithLongColName.selectExpr(\n",
    "    \"`This Long Column-Name`\",\n",
    "    \"`This Long Column-Name` as `new col`\").show(2)\n",
    "\n",
    "dfWithLongColName.select(expr(\"`This Long Column-Name`\")).columns\n",
    "\n",
    "# Removing Columns:\n",
    "df.drop(\"ORIGIN_COUNTRY_NAME\").columns\n",
    "dfWithLongColName.drop(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\") # Multiple Columns\n",
    "\n",
    "# Changing a Column’s Type (cast):\n",
    "df.withColumn(\"count2\", col(\"count\").cast(\"long\"))\n",
    "# SELECT *, cast(count as long) AS count2 FROM dfTable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b63ea4-56de-4fde-8033-06a23b7d61a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering:\n",
    "df.filter(col(\"count\") < 2).show(2)\n",
    "df.where(col(\"count\") < 2).where(col(\"ORIGIN_COUNTRY_NAME\") != \"Croatia\").show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c75cdc0-bbb9-4efe-b29a-e29d2aca1a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Getting Unique Rows\n",
    "df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().count() #256, like, SELECT COUNT(DISTINCT(ORIGIN_COUNTRY_NAME, DEST_COUNTRY_NAME)) FROM dfTable\n",
    "df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count() #125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e045ec-d2f7-49d6-a567-dcf3cffcd24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Samples: Sometimes, you might just want to sample some random records from your DataFrame.\n",
    "seed = 5\n",
    "withReplacement = False\n",
    "fraction = 0.5\n",
    "df.sample(withReplacement, fraction, seed).count()\n",
    "\n",
    "#Random Splits: Random splits can be helpful when you need to break up your DataFrame into a random “splits” of the original DataFrame.For Ml cases.\n",
    "dataFrames = df.randomSplit([0.25, 0.75], seed)\n",
    "dataFrames[0].count() > dataFrames[1].count() # False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c93944b-67de-4b21-943a-20c78d2d3e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending or Union of New rows:\n",
    "\n",
    "from pyspark.sql import Row\n",
    "schema = df.schema\n",
    "newRows = [\n",
    "  Row(\"New Country\", \"Other Country\", 5),\n",
    "  Row(\"New Country 2\", \"Other Country 3\", 1)\n",
    "]\n",
    "parallelizedRows = spark.sparkContext.parallelize(newRows) # for RDDs, massive data, this moves data to worker nodes, as new Rows lives in Driver node, by this, it splits and \n",
    "                                                           # distribute to worker nodes. for latest spark, for small set, not required to parallelize.\n",
    "newDF = spark.createDataFrame(parallelizedRows, schema)\n",
    "df.union(newDF)\\\n",
    "  .where(\"count = 1\")\\\n",
    "  .where(col(\"ORIGIN_COUNTRY_NAME\") != \"United States\")\\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13be6d70-10ae-4adf-8636-d8bc998b5ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting Rows:\n",
    "df.sort(\"count\").show(5)\n",
    "df.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(5)\n",
    "df.orderBy(col(\"count\"), col(\"DEST_COUNTRY_NAME\")).show(5)\n",
    "\n",
    "from pyspark.sql.functions import desc, asc\n",
    "df.orderBy(expr(\"count desc\")).show(2)\n",
    "df.orderBy(col(\"count\").desc(), col(\"DEST_COUNTRY_NAME\").asc()).show(2)\n",
    "\n",
    "# For optimization purposes, it’s sometimes advisable to sort within each partition before another set of transformations.\n",
    "spark.read.format(\"json\").load(\"./data/flight-data/json/2015-summary.json\").sortWithinPartitions(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40392d23-0653-45c4-a71c-920b7573a7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit\n",
    "df.limit(5).show()\n",
    "df.orderBy(expr(\"count desc\")).limit(6).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7a6655-3ae3-4266-ae04-1a26a423234e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition and Coalesce:\n",
    "'''\n",
    "Another important optimization opportunity is to partition the data according to some frequently\n",
    "filtered columns, which control the physical layout of data across the cluster including the\n",
    "partitioning scheme and the number of partitions.\n",
    "Repartition will incur a full shuffle of the data, regardless of whether one is necessary.\n",
    "'''\n",
    "\n",
    "df.rdd.getNumPartitions() # 1\n",
    "df.repartition(5)\n",
    "df.repartition(col(\"DEST_COUNTRY_NAME\")) # column based partitioning, if that columnis used for filtering often.\n",
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\"))\n",
    "\n",
    "# Coalesce: to combine partitions by reshuffling the partitions.\n",
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(2) #This operation will shuffle your data into five partitions based on the destination country name, and\n",
    "# then coalesce them (without a full shuffle):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b5f581-ccd9-4009-a0b1-16ec75144872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect():\n",
    "'''\n",
    "Spark maintains the state of the cluster in the driver. There are\n",
    "times when you’ll want to collect some of your data to the driver in order to manipulate it on\n",
    "your local machine.\n",
    "'''\n",
    "\n",
    "collectDF = df.limit(10)\n",
    "collectDF.take(5) # take works with an Integer count\n",
    "collectDF.show() # this prints it out nicely\n",
    "collectDF.show(5, False)\n",
    "collectDF.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6d515e-31b9-492a-875c-6584da31764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "collectDF.toLocalIterator() #collects partitions to the driver as an iterator,allows you to iterate over the entire dataset partition-by-partition in a serial manner.\n",
    "# collects brings all partitions to Driver, therefore, chances of OOM error, where as toLocalIterator, only one partition at a time. therfore, safer alternative to Collect().\n",
    "# Assume 'df' is a massive DataFrame with millions of rows\n",
    "iter_rows = df.toLocalIterator()\n",
    "\n",
    "# This loop only keeps one partition in memory at a time\n",
    "for row in iter_rows:\n",
    "    user_email = row[\"email\"]\n",
    "    # Perform a local Python action (e.g., calling an external API)\n",
    "    print(f\"Processing data for: {user_email}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
